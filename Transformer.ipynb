{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module) :\n",
    "     def __init__(self , vocab_size , d_model):\n",
    "         super().__init__()\n",
    "         self.d_model = d_model\n",
    "         self.embd = nn.Embedding(vocab_size, d_model)\n",
    "     def forward(self,X) : # X(batch_size , max_len)\n",
    "        return self.embd(X)*(np.sqrt(self.d_model)) # batch_size , max_len , d_model\n",
    "        \n",
    "class PositionEncoding(nn.Module) :\n",
    "    def __init__(self , max_len , d_model ,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.encodings = torch.zeros(max_len , d_model ,dtype  = torch.float)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for pos in range(max_len) : \n",
    "            for i in range(d_model) :\n",
    "                if(i%2):\n",
    "                    self.encodings[pos][i]  =  torch.cos(  torch.tensor( pos/(10000)**( (i-1) / d_model) , dtype  = torch.float32 )  )\n",
    "                else:\n",
    "                    self.encodings[pos][i]  =  torch.sin(torch.tensor(pos/(10000)**(i / d_model) , dtype = torch.float32))\n",
    "\n",
    "    def forward(self,input_embed) : # batch_size , max_len , d_model\n",
    "        return self.dropout((input_embed + input_embed))  # batch_size , max_len , d_model\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self , d_model , num_heads  , dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model , d_model , bias = False)\n",
    "        self.W_v = nn.Linear(d_model , d_model , bias = False)\n",
    "        self.W_k = nn.Linear(d_model , d_model , bias = False)\n",
    "        self.d_k = d_model//num_heads\n",
    "        self.W_o = nn.Linear(d_model , d_model )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "    def forward(self,q,k = None,v = None,mask = None):\n",
    "        if k == None : \n",
    "            k = q\n",
    "        if v == None :\n",
    "            v = q\n",
    "        \n",
    "        Query =  self.W_q(q) # batch , max_len , d_model  \n",
    "        key   =  self.W_k(k)\n",
    "        value =  self.W_v(v)\n",
    "        batch_size = q.shape[0]\n",
    "        Query = Query.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn_score = torch.matmul(Query ,Key.permute(0,1,3,2))/np.sqrt(d_k)\n",
    "        if mask is not None :\n",
    "            attn_scores.masked_fill_(mask == 0, float('-inf'))\n",
    "        attn_score = self.dropout(attn_score) #batch , h ,max_len ,max_len\n",
    "        attn_weight = torch.softmax(attn_score , dim = -1) #each token .and\n",
    "        output = torch.matmul(attn_weight , value)#batch , h ,max_len ,d_k\n",
    "        output = output.permute(0,2,1,3).contiguous() #batch,max_len,h,d_k\n",
    "        output.reshape(output.shape[0] , -1 , self.d_k*self.num_heads) #batch,max_len,d_model\n",
    "        return self.W_o(outputs) #batch,max_len,d_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_and_norm(nn.Module) :\n",
    "    def __init__(self ,d_model ,  dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "    def forward(self,x,output):\n",
    "        return self.layernorm(x + self.dropout(output) ) #batch,max_len,d_model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self , dff ,   d_model ,dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W1 = nn.Linear(d_model,dff)\n",
    "        self.W2 = nn.Linear(dff,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,X):\n",
    "        return self.W2(self.dropout(self.relu(self.W1(X)))) #batch,max_len,d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderblock(nn.Module):\n",
    "    def __init__(self , d_model , num_heads , dff , dropout = 0.1 ):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model , num_heads , dropout)\n",
    "        self.ffn = FFN(dff , d_model ,dropout)\n",
    "        self.residual1  = add_and_norm(d_model ,dropout)\n",
    "        self.residual2  = add_and_norm(d_model ,dropout)\n",
    "    def forward(self , x , mask):\n",
    "        attn = self.mha(x , mask = mask)\n",
    "        x = self.residual1(x , attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.residual2(x,ffn_output)\n",
    "        return x #batch,max_len,d_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self  ,  d_model , num_heads , dff , dropout = 0.1 , num_layers = 6 ):\n",
    "      super().__init__() \n",
    "      self.enc_layers = nn.ModuleList([Encoderblock(d_model,num_heads,dff,dropout)   for x in range(num_layers)] )\n",
    "    \n",
    "    def forward(self , x , mask = None):\n",
    "        for layer in self.enc_layers :\n",
    "            x = layer(x,mask)\n",
    "        return x    #batch,max_len,d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__( self , d_model , num_heads , dff , dropout = 0.1  ):\n",
    "        super().__init__() \n",
    "        self.masked_attn = MultiHeadAttention(d_model , num_heads , dropout = 0.1)\n",
    "        self.cross_attn =  MultiHeadAttention(d_model , num_heads , dropout = 0.1)\n",
    "        self.ffn = FFN(dff,d_model,dropout)\n",
    "        self.res1 = add_and_norm(d_model,dropout)\n",
    "        self.res2 = add_and_norm(d_model,dropout)\n",
    "        self.res3 = add_and_norm(d_model,dropout)\n",
    "    def forward(self , x , enc_output , enc_mask = None , dec_mask = None):\n",
    "        attn1 = self.masked_attn(x,mask = enc_mask)\n",
    "        x = self.res1(x,attn1)\n",
    "\n",
    "        attn2 = self.cross_attn(q = x,k = enc_output , v = enc_output , mask = dec_mask)\n",
    "        x = self.res2(x,attn2)\n",
    "\n",
    "        linear = self.ffn(x)\n",
    "        x = self.res3(x,linear)\n",
    "        return x #batch,max_len,d_model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self  ,  d_model , num_heads , dff , vocab_size , dropout = 0.1 , num_layers = 6 ):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.dec_layers = nn.ModuleList([DecoderBlock(d_model,num_heads,dff,dropout)   for x in range(num_layers)] )\n",
    "        self.linear = nn.Linear(d_model,vocab_size )\n",
    "    def forward(self,enc_output,enc_mask,dec_mask):\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x,enc_output,enc_mask,dec_mask)\n",
    "        return self.linear(x) #batch,max_len,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module): \n",
    "    def __init__(self ,src_vocab_size ,tgt_vocab_size , d_model , dff , max_len = 512 , num_heads = 8 , dropout = 0.1, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder_embedding = InputEmbeddings(src_vocab_size , d_model)\n",
    "        self.pos_enc = PositionEncoding(max_len,d_model,dropout)\n",
    "        self.encoder  = Encoder(d_model,num_heads,dff,dropout,num_layers) \n",
    "        self.decoder_embedding = InputEmbeddings(tgt_vocab_size,d_model)\n",
    "        self.pos_dec = PositionEncoding(max_len,d_model,dropout)\n",
    "        self.decoder = Decoder(d_model,num_heads,dff,tgt_vocab_size,dropout,num_layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 :\n",
    "                nn.init.xavier_uniform_(p) \n",
    "        \n",
    "    def forward(self,X,y,enc_mask = None,dec_mask = None):\n",
    "        enc_emb = self.encoder_embedding(X)\n",
    "        enc_emb = self.pos_enc(enc_emb)\n",
    "        dec_emb = self.decoder_embedding(y)\n",
    "        dec_emb = self.pos_dec(dec_emb)\n",
    "        enc_output = self.encoder(enc_emb , mask = enc_mask)\n",
    "        dec_output = self.decoder(dec_emb , enc_output , enc_mask , dec_mask)\n",
    "        return dec_output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset('wmt16', 'de-en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = dataset['train'].shuffle(seed=0).select(range(30000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_raw['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", errors = 'replace',\n",
    "                                         unk_token = '<UNK>',\n",
    "                                         bos_token = '<SOS>',\n",
    "                                         eos_token = '<EOS>',\n",
    "                                         pad_token = '<PAD>',\n",
    "                                         )\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    eng_sentences = [\"<SOS>\"+data[\"en\"]+\"<EOS>\" for data in batch[\"translation\"]]   \n",
    "    german_sentences = [\"<SOS>\"+data[\"de\"]+\"<EOS>\" for data in batch[\"translation\"]]\n",
    "    dec_tokenized = tokenizer(eng_sentences, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    enc_tokenized = tokenizer(german_sentences, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    return {\n",
    "        \"enc_input_ids\": enc_tokenized[\"input_ids\"],\n",
    "        \"enc_attention_mask\": enc_tokenized[\"attention_mask\"],\n",
    "        \"dec_input_ids\": dec_tokenized[\"input_ids\"],\n",
    "        \"dec_attention_mask\" : dec_tokenized[\"attention_mask\"],\n",
    "    }\n",
    "# train_raw_dataset = Dataset.from_dict(train_raw)\n",
    "tokenized_train_raw = train_raw.map(tokenize_function, batched=True, remove_columns=[\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOS Token:\", tokenizer.bos_token , tokenizer.bos_token_id)\n",
    "print(\"EOS Token:\", tokenizer.eos_token , tokenizer.eos_token_id)\n",
    "print(\"UNK Token:\", tokenizer.unk_token , tokenizer.unk_token_id)\n",
    "print(\"PAD Token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenized_train_raw))\n",
    "print(tokenizer.vocab_size)\n",
    "# model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# text = \"Dies ist ein Test.\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# output = model.generate(**inputs)\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.enc_input_ids = tokenized_dataset[\"enc_input_ids\"]\n",
    "        self.enc_attention_mask = tokenized_dataset[\"enc_attention_mask\"]\n",
    "        self.dec_input_ids = tokenized_dataset[\"dec_input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enc_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #teacher forcing\n",
    "        dec_input_ids = self.dec_input_ids[idx][:-1]\n",
    "        dec_target_ids = self.dec_input_ids[idx][1:]\n",
    "        seq_len = len(dec_input_ids)\n",
    "        dec_attention_mask = torch.tril(torch.ones(seq_len, seq_len)).to(dtype=torch.bool)\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": {\n",
    "                \"input_ids\": torch.tensor(self.enc_input_ids[idx], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(self.enc_attention_mask[idx], dtype=torch.long)\n",
    "            },\n",
    "            \"decoder_input\": {\n",
    "                \"attention_mask\": dec_attention_mask,\n",
    "                \"input_ids\": torch.tensor(dec_input_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(dec_target_ids, dtype=torch.long)\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.convert_ids_to_tokens(3485)\n",
    "print(\"Token for ID 3480:\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset  = TranslationDataset(tokenized_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = \"cuda\"  if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"decoder_input\"][\"labels\"]) \n",
    "print(train_dataset[0]['decoder_input']['input_ids'])\n",
    "print(train_dataset[0]['decoder_input']['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 3\n",
    "GRAD_CLIP = 1.0\n",
    "LAYERS = 6 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = transformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    dff=1024\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "# Warmup scheduler\n",
    "def lr_lambda(step, warmup_steps=4000):\n",
    "    if step < warmup_steps:\n",
    "        return step / warmup_steps\n",
    "    return (warmup_steps**0.5) * (step**-0.5)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\"):\n",
    "            enc_input_ids = batch['encoder_input']['input_ids'].to(device)\n",
    "            enc_mask = batch[\"encoder_input\"][\"attention_mask\"].to(device)\n",
    "            dec_input_ids = batch[\"decoder_input\"][\"input_ids\"].to(device)\n",
    "            dec_labels = batch[\"decoder_input\"][\"labels\"].to(device)\n",
    "            dec_mask = batch[\"decoder_input\"][\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(enc_input_ids, dec_input_ids, enc_mask, dec_mask)  # (batch_size, seq_len, vocab_size)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), dec_labels.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader)}\")\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
